{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, ElementClickInterceptedException\n",
    "import requests\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import urllib3\n",
    "import urllib.request\n",
    "#import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the Saree images path and its titlefrom Amazon site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_links = ['https://www.amazon.in/s?k=saree&ref=nb_sb_noss','https://www.amazon.in/s?k=saree&page=2&qid=1623300883&ref=sr_pg_2','https://www.amazon.in/s?k=saree&page=3&qid=1623304127&ref=sr_pg_3','https://www.amazon.in/s?k=saree&page=4&qid=1623304133&ref=sr_pg_4','https://www.amazon.in/s?k=saree&page=5&qid=1623331918&ref=sr_pg_5']\n",
    "j_links = ['https://www.amazon.in/s?k=jeans+for+men+stylish&crid=1HP2F7L5NFFWR&sprefix=jeans%2Caps%2C312&ref=nb_sb_ss_ts-doa-p_1_5','https://www.amazon.in/s?k=jeans+for+men+stylish&page=2&crid=1HP2F7L5NFFWR&qid=1623304464&sprefix=jeans%2Caps%2C312&ref=sr_pg_2','https://www.amazon.in/s?k=jeans+for+men+stylish&page=3&crid=1HP2F7L5NFFWR&qid=1623304518&sprefix=jeans%2Caps%2C312&ref=sr_pg_3','https://www.amazon.in/s?k=jeans+for+men+stylish&page=4&crid=1HP2F7L5NFFWR&qid=1623304569&sprefix=jeans%2Caps%2C312&ref=sr_pg_4']\n",
    "t_links = ['https://www.amazon.in/s?k=trousers+for+men&ref=nb_sb_noss_2','https://www.amazon.in/s?k=trousers+for+men&page=2&qid=1623304681&ref=sr_pg_2','https://www.amazon.in/s?k=trousers+for+men&page=3&qid=1623304700&ref=sr_pg_3','https://www.amazon.in/s?k=trousers+for+men&page=4&qid=1623304728&ref=sr_pg_4']\n",
    "Saree_path = 'C:/Users/735184/OneDrive - Cognizant/Desktop/Digvijay/Image_Classification/Saree/'\n",
    "Jeans_path = 'C:/Users/735184/OneDrive - Cognizant/Desktop/Digvijay/Image_Classification/Jeans/'\n",
    "Trousers_path = 'C:/Users/735184/OneDrive - Cognizant/Desktop/Digvijay/Image_Classification/Trousers/'\n",
    "\n",
    "img_url_jeans = []\n",
    "img_url_jeans_title = []\n",
    "img_url_trousers = []\n",
    "img_url_trousers_title = []\n",
    "img_url_saree = []\n",
    "img_url_saree_title = []\n",
    "\n",
    "\n",
    "def extract_images_amz (link_list,list_img,List_title):\n",
    "    driver=webdriver.Chrome('C://Users//735184//Downloads//chromedriver_win32_New//chromedriver.exe')\n",
    "    driver.maximize_window()\n",
    "    time.sleep(1)\n",
    "    for i in link_list:\n",
    "        driver.get(i)\n",
    "        image_cls = driver.find_elements_by_xpath(\"//img[@class='s-image']\")\n",
    "        image_title = driver.find_elements_by_xpath(\"//span[@class='a-size-base-plus a-color-base a-text-normal']\")\n",
    "        for i in image_cls:\n",
    "            list_img.append(i.get_attribute('src'))\n",
    "        for i in image_title:\n",
    "            List_title.append(i.text)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the images at particular location on desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_images_amz(s_links,img_url_saree,img_url_saree_title)\n",
    "time.sleep(5)\n",
    "for i in range(len(img_url_saree[:241])):\n",
    "    nm = 'Saree_'+str(i)+'.jpg'\n",
    "    image = urllib.request.URLopener()\n",
    "    image.retrieve(img_url_saree[i],Saree_path+nm)\n",
    "time.sleep(15)\n",
    "extract_images_amz(j_links,img_url_jeans,img_url_jeans_title)\n",
    "time.sleep(5)\n",
    "for i in range(len(img_url_jeans[:241])):\n",
    "    nm = 'Jeans_'+str(i)+'.jpg'\n",
    "    image = urllib.request.URLopener()\n",
    "    image.retrieve(img_url_jeans[i],Jeans_path+nm)\n",
    "time.sleep(15)\n",
    "extract_images_amz(t_links,img_url_trousers,img_url_trousers_title)\n",
    "for i in range(len(img_url_trousers[:241])):\n",
    "    nm = 'Trousers_'+str(i)+'.jpg'\n",
    "    image = urllib.request.URLopener()\n",
    "    image.retrieve(img_url_trousers[i],Trousers_path+nm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Start with the Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "  \n",
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = 'C:/Users/735184/OneDrive - Cognizant/Desktop/Digvijay/Image_Class_Project/Image_Classification/'\n",
    "validation_image = 'C:/Users/735184/OneDrive - Cognizant/Desktop/Digvijay/Image_Class_Project/Validation'\n",
    "\n",
    "nb_train_samples =663\n",
    "nb_validation_samples = 57\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (2, 2), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "model.add(Conv2D(32, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "model.add(Conv2D(64, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 663 images belonging to 3 classes.\n",
      "Found 57 images belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "41/41 [==============================] - 19s 452ms/step - loss: -1.4124 - accuracy: 0.3338 - val_loss: -14.1838 - val_accuracy: 0.3125\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 20s 482ms/step - loss: -28.8241 - accuracy: 0.3431 - val_loss: 12.7645 - val_accuracy: 0.2927\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 20s 485ms/step - loss: -181.6719 - accuracy: 0.3338 - val_loss: -736.2914 - val_accuracy: 0.3659\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 22s 530ms/step - loss: -703.4667 - accuracy: 0.3308 - val_loss: 981.5819 - val_accuracy: 0.4146\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 22s 531ms/step - loss: -1044.7633 - accuracy: 0.3462 - val_loss: 616.8278 - val_accuracy: 0.3542\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 22s 547ms/step - loss: -2135.2156 - accuracy: 0.3570 - val_loss: -590.2169 - val_accuracy: 0.4634\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 22s 530ms/step - loss: -5530.1742 - accuracy: 0.3400 - val_loss: -254.5039 - val_accuracy: 0.3171\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 23s 561ms/step - loss: -9404.8520 - accuracy: 0.3478 - val_loss: 18682.9824 - val_accuracy: 0.2683\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 21s 511ms/step - loss: -15692.8075 - accuracy: 0.3385 - val_loss: -25542.7773 - val_accuracy: 0.3750\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 22s 536ms/step - loss: -21989.8158 - accuracy: 0.3476 - val_loss: -41750.2539 - val_accuracy: 0.3902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1fcc92d04c8>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "  \n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "  \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_image,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "  \n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_image,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "  \n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_saved.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
